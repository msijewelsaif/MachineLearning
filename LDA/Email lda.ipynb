{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ed0a81-28f2-42b7-8df9-c307905266bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21dbc01-334a-4624-b3b9-29d7f27dfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('email.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a15179-3b0f-439d-ae80-379547ff7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5573 entries, 0 to 5572\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5573 non-null   object\n",
      " 1   Message   5573 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f782080-7042-498c-ab0e-83b4b7333bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a251a928-c690-461a-b079-ae7c57620bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "hist method requires numerical or datetime columns, nothing to plot.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[38;5;241m.\u001b[39mhist()\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\pandas\\plotting\\_core.py:251\u001b[0m, in \u001b[0;36mhist_frame\u001b[1;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, backend, legend, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mMake a histogram of the DataFrame's columns.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    >>> hist = df.hist(bins=3)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m plot_backend \u001b[38;5;241m=\u001b[39m _get_plot_backend(backend)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mhist_frame(\n\u001b[0;32m    252\u001b[0m     data,\n\u001b[0;32m    253\u001b[0m     column\u001b[38;5;241m=\u001b[39mcolumn,\n\u001b[0;32m    254\u001b[0m     by\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m    255\u001b[0m     grid\u001b[38;5;241m=\u001b[39mgrid,\n\u001b[0;32m    256\u001b[0m     xlabelsize\u001b[38;5;241m=\u001b[39mxlabelsize,\n\u001b[0;32m    257\u001b[0m     xrot\u001b[38;5;241m=\u001b[39mxrot,\n\u001b[0;32m    258\u001b[0m     ylabelsize\u001b[38;5;241m=\u001b[39mylabelsize,\n\u001b[0;32m    259\u001b[0m     yrot\u001b[38;5;241m=\u001b[39myrot,\n\u001b[0;32m    260\u001b[0m     ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m    261\u001b[0m     sharex\u001b[38;5;241m=\u001b[39msharex,\n\u001b[0;32m    262\u001b[0m     sharey\u001b[38;5;241m=\u001b[39msharey,\n\u001b[0;32m    263\u001b[0m     figsize\u001b[38;5;241m=\u001b[39mfigsize,\n\u001b[0;32m    264\u001b[0m     layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    265\u001b[0m     legend\u001b[38;5;241m=\u001b[39mlegend,\n\u001b[0;32m    266\u001b[0m     bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    268\u001b[0m )\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\hist.py:549\u001b[0m, in \u001b[0;36mhist_frame\u001b[1;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, legend, **kwds)\u001b[0m\n\u001b[0;32m    546\u001b[0m naxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m naxes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist method requires numerical or datetime columns, nothing to plot.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m     )\n\u001b[0;32m    553\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m create_subplots(\n\u001b[0;32m    554\u001b[0m     naxes\u001b[38;5;241m=\u001b[39mnaxes,\n\u001b[0;32m    555\u001b[0m     ax\u001b[38;5;241m=\u001b[39max,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    560\u001b[0m     layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    561\u001b[0m )\n\u001b[0;32m    562\u001b[0m _axes \u001b[38;5;241m=\u001b[39m flatten_axes(axes)\n",
      "\u001b[1;31mValueError\u001b[0m: hist method requires numerical or datetime columns, nothing to plot."
     ]
    }
   ],
   "source": [
    "data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225a9a8a-e77a-4acd-a47c-d2e833995e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files (you might need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901578d0-def1-480a-af6a-af550a0aa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('email.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62b79f88-1607-4011-825e-175f497225d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stop words and punctuation, and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing\n",
    "data['Processed_Message'] = data['Message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a609ec4b-778b-4ab7-a1f0-2b84ed3f15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['Processed_Message'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['Processed_Message']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f76921a-030e-43c3-bfa2-bf021aeb5f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.016*\"need\" + 0.014*\"like\" + 0.013*\"get\" + 0.012*\"know\"')\n",
      "(1, '0.051*\"gt\" + 0.051*\"lt\" + 0.046*\"call\" + 0.017*\"claim\"')\n",
      "(2, '0.035*\"u\" + 0.022*\"ü\" + 0.021*\"lor\" + 0.020*\"go\"')\n",
      "(3, '0.022*\"got\" + 0.015*\"hi\" + 0.015*\"something\" + 0.012*\"fine\"')\n",
      "(4, '0.023*\"message\" + 0.020*\"right\" + 0.020*\"pls\" + 0.018*\"k\"')\n",
      "(5, '0.090*\"u\" + 0.015*\"r\" + 0.013*\"going\" + 0.013*\"na\"')\n",
      "(6, '0.045*\"ur\" + 0.019*\"txt\" + 0.018*\"tone\" + 0.013*\"u\"')\n",
      "(7, '0.055*\"call\" + 0.049*\"free\" + 0.026*\"text\" + 0.021*\"mobile\"')\n",
      "(8, '0.016*\"know\" + 0.012*\"chat\" + 0.010*\"babe\" + 0.010*\"find\"')\n",
      "(9, '0.035*\"good\" + 0.026*\"day\" + 0.015*\"love\" + 0.015*\"dear\"')\n"
     ]
    }
   ],
   "source": [
    "# Set the number of topics you want to extract\n",
    "num_topics = 10\n",
    "\n",
    "# Create LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d7330fb-2c69-489c-9570-424a5f987a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"call\" + 0.048*\"free\" + 0.025*\"text\" + 0.022*\"mobile\"')\n",
      "(1, '0.027*\"know\" + 0.025*\"k\" + 0.021*\"let\" + 0.020*\"hi\"')\n",
      "(2, '0.018*\"one\" + 0.015*\"thanks\" + 0.012*\"keep\" + 0.011*\"see\"')\n",
      "(3, '0.017*\"u\" + 0.016*\"call\" + 0.012*\"stop\" + 0.009*\"one\"')\n",
      "(4, '0.025*\"na\" + 0.021*\"da\" + 0.018*\"call\" + 0.015*\"want\"')\n",
      "(5, '0.053*\"gt\" + 0.052*\"lt\" + 0.023*\"good\" + 0.023*\"love\"')\n",
      "(6, '0.032*\"ok\" + 0.011*\"meeting\" + 0.011*\"get\" + 0.010*\"work\"')\n",
      "(7, '0.066*\"u\" + 0.022*\"ü\" + 0.021*\"lor\" + 0.016*\"home\"')\n",
      "(8, '0.017*\"got\" + 0.013*\"get\" + 0.012*\"right\" + 0.011*\"time\"')\n",
      "(9, '0.075*\"u\" + 0.031*\"ur\" + 0.024*\"call\" + 0.023*\"r\"')\n"
     ]
    }
   ],
   "source": [
    "# Set the number of topics you want to extract\n",
    "num_topics = 10\n",
    "\n",
    "# Create LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=50)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27c5c87f-1677-4d23-9365-b84b4e0728aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.054*\"k\" + 0.052*\"na\" + 0.047*\"home\" + 0.045*\"week\" + 0.032*\"get\" + 0.030*\"gon\" + 0.020*\"tell\" + 0.016*\"ur\" + 0.015*\"every\" + 0.013*\"wan\"')\n",
      "(1, '0.126*\"call\" + 0.052*\"please\" + 0.039*\"number\" + 0.037*\"ok\" + 0.022*\"cash\" + 0.018*\"service\" + 0.017*\"meeting\" + 0.017*\"chat\" + 0.016*\"customer\" + 0.015*\"landline\"')\n",
      "(2, '0.034*\"good\" + 0.030*\"going\" + 0.026*\"b\" + 0.025*\"way\" + 0.023*\"get\" + 0.022*\"could\" + 0.021*\"need\" + 0.019*\"yeah\" + 0.018*\"problem\" + 0.018*\"next\"')\n",
      "(3, '0.163*\"u\" + 0.044*\"lor\" + 0.029*\"r\" + 0.025*\"e\" + 0.023*\"ok\" + 0.021*\"got\" + 0.021*\"ask\" + 0.020*\"go\" + 0.020*\"already\" + 0.016*\"wat\"')\n",
      "(4, '0.025*\"ya\" + 0.024*\"u\" + 0.020*\"dream\" + 0.019*\"x\" + 0.017*\"amp\" + 0.017*\"without\" + 0.015*\"true\" + 0.014*\"heart\" + 0.013*\"food\" + 0.011*\"set\"')\n",
      "(5, '0.025*\"plan\" + 0.024*\"mean\" + 0.022*\"get\" + 0.019*\"ur\" + 0.017*\"use\" + 0.017*\"word\" + 0.016*\"take\" + 0.014*\"enjoy\" + 0.013*\"part\" + 0.012*\"next\"')\n",
      "(6, '0.055*\"u\" + 0.053*\"take\" + 0.027*\"care\" + 0.017*\"try\" + 0.016*\"yo\" + 0.014*\"something\" + 0.011*\"dont\" + 0.011*\"else\" + 0.011*\"de\" + 0.010*\"go\"')\n",
      "(7, '0.094*\"u\" + 0.040*\"ü\" + 0.035*\"come\" + 0.034*\"da\" + 0.023*\"n\" + 0.021*\"ur\" + 0.016*\"ok\" + 0.016*\"wan\" + 0.014*\"wat\" + 0.014*\"go\"')\n",
      "(8, '0.055*\"yes\" + 0.041*\"leave\" + 0.032*\"gud\" + 0.029*\"v\" + 0.024*\"one\" + 0.023*\"waiting\" + 0.022*\"sir\" + 0.019*\"ur\" + 0.019*\"big\" + 0.018*\"anything\"')\n",
      "(9, '0.033*\"time\" + 0.031*\"well\" + 0.031*\"night\" + 0.030*\"much\" + 0.027*\"love\" + 0.023*\"babe\" + 0.021*\"good\" + 0.017*\"hope\" + 0.017*\"lol\" + 0.016*\"like\"')\n",
      "(10, '0.089*\"know\" + 0.039*\"let\" + 0.036*\"hi\" + 0.027*\"still\" + 0.026*\"dont\" + 0.023*\"also\" + 0.019*\"around\" + 0.016*\"dinner\" + 0.014*\"like\" + 0.014*\"guy\"')\n",
      "(11, '0.036*\"ur\" + 0.032*\"claim\" + 0.029*\"call\" + 0.027*\"prize\" + 0.026*\"txt\" + 0.020*\"u\" + 0.018*\"c\" + 0.018*\"contact\" + 0.017*\"mobile\" + 0.016*\"win\"')\n",
      "(12, '0.083*\"free\" + 0.058*\"text\" + 0.055*\"stop\" + 0.039*\"reply\" + 0.038*\"call\" + 0.034*\"mobile\" + 0.027*\"phone\" + 0.025*\"message\" + 0.023*\"txt\" + 0.022*\"nokia\"')\n",
      "(13, '0.063*\"sorry\" + 0.057*\"later\" + 0.051*\"want\" + 0.031*\"call\" + 0.021*\"buy\" + 0.020*\"stuff\" + 0.018*\"tonight\" + 0.018*\"yup\" + 0.014*\"done\" + 0.013*\"probably\"')\n",
      "(14, '0.036*\"see\" + 0.035*\"think\" + 0.027*\"many\" + 0.026*\"friend\" + 0.021*\"u\" + 0.017*\"thought\" + 0.016*\"important\" + 0.016*\"looking\" + 0.014*\"want\" + 0.012*\"drive\"')\n",
      "(15, '0.048*\"pls\" + 0.047*\"oh\" + 0.037*\"right\" + 0.030*\"send\" + 0.028*\"thanks\" + 0.026*\"great\" + 0.026*\"cant\" + 0.025*\"pick\" + 0.023*\"late\" + 0.023*\"back\"')\n",
      "(16, '0.125*\"gt\" + 0.124*\"lt\" + 0.015*\"ur\" + 0.012*\"birthday\" + 0.012*\"office\" + 0.010*\"min\" + 0.010*\"day\" + 0.009*\"decimal\" + 0.009*\"today\" + 0.008*\"money\"')\n",
      "(17, '0.029*\"dear\" + 0.023*\"life\" + 0.019*\"day\" + 0.018*\"get\" + 0.018*\"love\" + 0.015*\"boy\" + 0.014*\"feel\" + 0.014*\"box\" + 0.014*\"good\" + 0.013*\"like\"')\n",
      "(18, '0.042*\"show\" + 0.029*\"cool\" + 0.026*\"account\" + 0.022*\"point\" + 0.020*\"put\" + 0.018*\"call\" + 0.017*\"thats\" + 0.016*\"lei\" + 0.015*\"trying\" + 0.014*\"private\"')\n",
      "(19, '0.045*\"day\" + 0.036*\"happy\" + 0.036*\"work\" + 0.029*\"year\" + 0.024*\"go\" + 0.023*\"went\" + 0.020*\"new\" + 0.019*\"hey\" + 0.019*\"even\" + 0.018*\"lunch\"')\n"
     ]
    }
   ],
   "source": [
    "# Set the number of topics you want to extract\n",
    "num_topics = 20\n",
    "\n",
    "# Create LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=30)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50e5e2-d637-40a2-8edf-3a0e7837a484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
