{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "724a984c-a2b9-42b5-a4ee-921a57c3d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Md Saiful\n",
      "[nltk_data]     Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3cddfbb-2dd9-4985-a302-ee2c8d810de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('email.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b70bed-7454-43e5-8290-9bc3eec953bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stop words and punctuation, and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123a74d6-006f-406e-8008-ed1ce250acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "data['Processed_Message'] = data['Message'].apply(preprocess_text)\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['Processed_Message'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['Processed_Message']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7708fb1-3145-41f9-a8fa-82b727b924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: u (0.0326) + free (0.0239) + call (0.0221) + text (0.0151) + ur (0.0148) + stop (0.0143) + txt (0.0133) + reply (0.0112) + mobile (0.0109) + get (0.0108)\n",
      "Topic 1: call (0.0482) + claim (0.0132) + prize (0.0131) + ok (0.0128) + number (0.0106) + please (0.0094) + urgent (0.0093) + cash (0.0083) + time (0.0078) + customer (0.0076)\n",
      "Topic 2: u (0.0225) + good (0.0191) + day (0.0134) + love (0.0098) + know (0.0098) + happy (0.0081) + ur (0.0080) + dear (0.0077) + night (0.0074) + need (0.0073)\n",
      "Topic 3: u (0.0454) + go (0.0184) + Ã¼ (0.0146) + lor (0.0138) + da (0.0130) + got (0.0118) + n (0.0100) + home (0.0095) + wat (0.0092) + come (0.0080)\n",
      "Topic 4: gt (0.0264) + lt (0.0262) + come (0.0129) + get (0.0125) + like (0.0117) + pls (0.0081) + want (0.0081) + still (0.0076) + got (0.0076) + know (0.0076)\n"
     ]
    }
   ],
   "source": [
    "# Apply LDA\n",
    "num_topics = 5  # Choose a number of topics\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Display topics\n",
    "def display_topics(model, num_words):\n",
    "    topics = model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "    for topic in topics:\n",
    "        print(f\"Topic {topic[0]}: \" + \" + \".join([f\"{word[0]} ({word[1]:.4f})\" for word in topic[1]]))\n",
    "\n",
    "# Display the top words for each topic\n",
    "num_top_words = 10\n",
    "display_topics(lda_model, num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ef922-b6a7-4311-abbc-bbaf3386861c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
