{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe56f8c-22ee-47b3-bde0-3c788a8fab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.170182004570961\n",
      "Epoch 2/50, Loss: 0.169032484292984\n",
      "Epoch 3/50, Loss: 0.16788168251514435\n",
      "Epoch 4/50, Loss: 0.16672958433628082\n",
      "Epoch 5/50, Loss: 0.16557620465755463\n",
      "Epoch 6/50, Loss: 0.16442155838012695\n",
      "Epoch 7/50, Loss: 0.1632656455039978\n",
      "Epoch 8/50, Loss: 0.16210836172103882\n",
      "Epoch 9/50, Loss: 0.16094955801963806\n",
      "Epoch 10/50, Loss: 0.1597890406847\n",
      "Epoch 11/50, Loss: 0.15862677991390228\n",
      "Epoch 12/50, Loss: 0.1574627012014389\n",
      "Epoch 13/50, Loss: 0.15629680454730988\n",
      "Epoch 14/50, Loss: 0.15512916445732117\n",
      "Epoch 15/50, Loss: 0.15395987033843994\n",
      "Epoch 16/50, Loss: 0.15278904139995575\n",
      "Epoch 17/50, Loss: 0.15161679685115814\n",
      "Epoch 18/50, Loss: 0.15044328570365906\n",
      "Epoch 19/50, Loss: 0.14926868677139282\n",
      "Epoch 20/50, Loss: 0.1480931043624878\n",
      "Epoch 21/50, Loss: 0.1469167321920395\n",
      "Epoch 22/50, Loss: 0.14573973417282104\n",
      "Epoch 23/50, Loss: 0.1445622742176056\n",
      "Epoch 24/50, Loss: 0.14338447153568268\n",
      "Epoch 25/50, Loss: 0.14220654964447021\n",
      "Epoch 26/50, Loss: 0.14102859795093536\n",
      "Epoch 27/50, Loss: 0.13985078036785126\n",
      "Epoch 28/50, Loss: 0.13867329061031342\n",
      "Epoch 29/50, Loss: 0.13749626278877258\n",
      "Epoch 30/50, Loss: 0.1363198310136795\n",
      "Epoch 31/50, Loss: 0.1351442039012909\n",
      "Epoch 32/50, Loss: 0.1339695155620575\n",
      "Epoch 33/50, Loss: 0.13279592990875244\n",
      "Epoch 34/50, Loss: 0.13162361085414886\n",
      "Epoch 35/50, Loss: 0.13045275211334229\n",
      "Epoch 36/50, Loss: 0.12928351759910583\n",
      "Epoch 37/50, Loss: 0.12811605632305145\n",
      "Epoch 38/50, Loss: 0.12695056200027466\n",
      "Epoch 39/50, Loss: 0.12578721344470978\n",
      "Epoch 40/50, Loss: 0.12462617456912994\n",
      "Epoch 41/50, Loss: 0.12346763908863068\n",
      "Epoch 42/50, Loss: 0.12231174111366272\n",
      "Epoch 43/50, Loss: 0.12115869671106339\n",
      "Epoch 44/50, Loss: 0.12000864744186401\n",
      "Epoch 45/50, Loss: 0.11886179447174072\n",
      "Epoch 46/50, Loss: 0.11771830916404724\n",
      "Epoch 47/50, Loss: 0.11657832562923431\n",
      "Epoch 48/50, Loss: 0.11544205248355865\n",
      "Epoch 49/50, Loss: 0.11430966854095459\n",
      "Epoch 50/50, Loss: 0.11318130791187286\n",
      "Topic 0:\n",
      "  allocation\n",
      "  tasty\n",
      "  type\n",
      "  topic\n",
      "  networks\n",
      "  documents\n",
      "  neural\n",
      "  modeling\n",
      "  sample\n",
      "  bangladeshi\n",
      "Category: Machine Learning\n",
      "Topic 1:\n",
      "  learning\n",
      "  used\n",
      "  love\n",
      "  model\n",
      "  method\n",
      "  tasty\n",
      "  popular\n",
      "  sample\n",
      "  networks\n",
      "  bangladeshi\n",
      "Category: Machine Learning\n",
      "Topic 2:\n",
      "  cuisine\n",
      "  topic\n",
      "  love\n",
      "  topics\n",
      "  popular\n",
      "  type\n",
      "  allocation\n",
      "  machine\n",
      "  document\n",
      "  latent\n",
      "Category: Machine Learning\n",
      "Topic 3:\n",
      "  machine\n",
      "  bangladeshi\n",
      "  food\n",
      "  neural\n",
      "  used\n",
      "  learning\n",
      "  type\n",
      "  latent\n",
      "  cuisine\n",
      "  document\n",
      "Category: Machine Learning\n",
      "Topic 4:\n",
      "  document\n",
      "  sample\n",
      "  cuisine\n",
      "  type\n",
      "  learning\n",
      "  used\n",
      "  food\n",
      "  topics\n",
      "  popular\n",
      "  machine\n",
      "Category: Machine Learning\n",
      "Topic 5:\n",
      "  used\n",
      "  modeling\n",
      "  food\n",
      "  machine\n",
      "  networks\n",
      "  tasty\n",
      "  bangladeshi\n",
      "  allocation\n",
      "  popular\n",
      "  learning\n",
      "Category: Machine Learning\n",
      "Topic 6:\n",
      "  cuisine\n",
      "  love\n",
      "  bangladeshi\n",
      "  networks\n",
      "  modeling\n",
      "  sample\n",
      "  learning\n",
      "  used\n",
      "  tasty\n",
      "  food\n",
      "Category: Machine Learning\n",
      "Topic 7:\n",
      "  topic\n",
      "  topics\n",
      "  dirichlet\n",
      "  machine\n",
      "  latent\n",
      "  model\n",
      "  allocation\n",
      "  used\n",
      "  popular\n",
      "  love\n",
      "Category: Machine Learning\n",
      "Topic 8:\n",
      "  used\n",
      "  tasty\n",
      "  learning\n",
      "  model\n",
      "  machine\n",
      "  networks\n",
      "  neural\n",
      "  bangladeshi\n",
      "  method\n",
      "  latent\n",
      "Category: Machine Learning\n",
      "Topic 9:\n",
      "  tasty\n",
      "  bangladeshi\n",
      "  document\n",
      "  dirichlet\n",
      "  love\n",
      "  latent\n",
      "  learning\n",
      "  food\n",
      "  modeling\n",
      "  topics\n",
      "Category: Machine Learning\n",
      "Topic 10:\n",
      "  sample\n",
      "  cuisine\n",
      "  networks\n",
      "  allocation\n",
      "  neural\n",
      "  document\n",
      "  love\n",
      "  topic\n",
      "  learning\n",
      "  model\n",
      "Category: Machine Learning\n",
      "Topic 11:\n",
      "  dirichlet\n",
      "  popular\n",
      "  method\n",
      "  latent\n",
      "  used\n",
      "  modeling\n",
      "  allocation\n",
      "  food\n",
      "  topics\n",
      "  model\n",
      "Category: Topic Modeling\n",
      "Topic 12:\n",
      "  type\n",
      "  bangladeshi\n",
      "  networks\n",
      "  allocation\n",
      "  model\n",
      "  document\n",
      "  popular\n",
      "  topics\n",
      "  documents\n",
      "  food\n",
      "Category: Machine Learning\n",
      "Topic 13:\n",
      "  method\n",
      "  cuisine\n",
      "  used\n",
      "  latent\n",
      "  documents\n",
      "  machine\n",
      "  dirichlet\n",
      "  bangladeshi\n",
      "  topics\n",
      "  love\n",
      "Category: Machine Learning\n",
      "Topic 14:\n",
      "  topic\n",
      "  documents\n",
      "  sample\n",
      "  type\n",
      "  model\n",
      "  document\n",
      "  food\n",
      "  machine\n",
      "  love\n",
      "  popular\n",
      "Category: Machine Learning\n",
      "Topic 15:\n",
      "  topics\n",
      "  type\n",
      "  learning\n",
      "  machine\n",
      "  allocation\n",
      "  networks\n",
      "  document\n",
      "  method\n",
      "  cuisine\n",
      "  dirichlet\n",
      "Category: Machine Learning\n",
      "Topic 16:\n",
      "  method\n",
      "  allocation\n",
      "  networks\n",
      "  topic\n",
      "  latent\n",
      "  love\n",
      "  food\n",
      "  sample\n",
      "  documents\n",
      "  bangladeshi\n",
      "Category: Machine Learning\n",
      "Topic 17:\n",
      "  allocation\n",
      "  bangladeshi\n",
      "  love\n",
      "  document\n",
      "  documents\n",
      "  learning\n",
      "  used\n",
      "  model\n",
      "  type\n",
      "  modeling\n",
      "Category: Machine Learning\n",
      "Topic 18:\n",
      "  love\n",
      "  bangladeshi\n",
      "  documents\n",
      "  topics\n",
      "  model\n",
      "  food\n",
      "  sample\n",
      "  learning\n",
      "  tasty\n",
      "  machine\n",
      "Category: Machine Learning\n",
      "Topic 19:\n",
      "  food\n",
      "  allocation\n",
      "  method\n",
      "  dirichlet\n",
      "  document\n",
      "  tasty\n",
      "  documents\n",
      "  bangladeshi\n",
      "  learning\n",
      "  topic\n",
      "Category: Machine Learning\n",
      "Topic 20:\n",
      "  topics\n",
      "  cuisine\n",
      "  type\n",
      "  used\n",
      "  latent\n",
      "  topic\n",
      "  bangladeshi\n",
      "  dirichlet\n",
      "  machine\n",
      "  sample\n",
      "Category: Machine Learning\n",
      "Topic 21:\n",
      "  type\n",
      "  modeling\n",
      "  networks\n",
      "  topics\n",
      "  learning\n",
      "  sample\n",
      "  model\n",
      "  latent\n",
      "  love\n",
      "  neural\n",
      "Category: Machine Learning\n",
      "Topic 22:\n",
      "  used\n",
      "  documents\n",
      "  latent\n",
      "  modeling\n",
      "  love\n",
      "  sample\n",
      "  food\n",
      "  bangladeshi\n",
      "  topic\n",
      "  topics\n",
      "Category: Topic Modeling\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class NeuralLDA(nn.Module):\n",
    "    def __init__(self, num_topics, input_dim):\n",
    "        super(NeuralLDA, self).__init__()\n",
    "        self.num_topics = num_topics\n",
    "        self.input_dim = input_dim\n",
    "        self.topic_word_matrix = nn.Parameter(torch.randn(num_topics, input_dim))\n",
    "        self.topic_prior = nn.Parameter(torch.randn(num_topics))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        topic_word_matrix = torch.softmax(self.topic_word_matrix, dim=1)\n",
    "        topic_prior = torch.softmax(self.topic_prior, dim=0)\n",
    "        reconstruction = torch.matmul(X, topic_word_matrix.T)\n",
    "        return reconstruction\n",
    "\n",
    "def train_model(X_tensor, model, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction = model(X_tensor)\n",
    "        loss = nn.MSELoss()(reconstruction, X_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is a sample document about machine learning.\",\n",
    "    \"Neural networks are a type of machine learning model.\",\n",
    "    \"Topic modeling can be used to find topics in documents.\",\n",
    "    \"Latent Dirichlet Allocation is a popular topic modeling method.\",\n",
    "    \"We love tasty food.\",\n",
    "    \"bangladeshi cuisine.\",\n",
    "]\n",
    "\n",
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Define model and optimizer\n",
    "num_topics = X_tensor.shape[1]  # Use the number of features as num_topics\n",
    "model = NeuralLDA(num_topics=num_topics, input_dim=X_tensor.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(X_tensor, model, optimizer)\n",
    "\n",
    "# Get the topic-word distributions\n",
    "topic_word_distributions = model.topic_word_matrix.detach().numpy()\n",
    "\n",
    "# Define a function to categorize topics based on top words\n",
    "def categorize_topic(topic_dist, top_words, vectorizer):\n",
    "    top_word_indices = np.argsort(topic_dist)[::-1][:10]\n",
    "    top_words_list = [vectorizer.get_feature_names_out()[i] for i in top_word_indices]\n",
    "    \n",
    "    # Define your categories based on top words\n",
    "    categories = {\n",
    "        'Machine Learning': ['machine', 'learning', 'neural', 'networks'],\n",
    "        'Topic Modeling': ['topic', 'modeling', 'documents', 'allocation'],\n",
    "        'Food': ['food', 'eat', 'tasty', 'crispy','cusine'],\n",
    "        \n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in top_words_list for word in keywords):\n",
    "            return category\n",
    "    return 'Uncategorized'\n",
    "\n",
    "# Print topic-word distributions and their categories\n",
    "for topic_idx, topic_dist in enumerate(topic_word_distributions):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words = np.argsort(topic_dist)[::-1][:10]\n",
    "    for word_idx in top_words:\n",
    "        print(f\"  {vectorizer.get_feature_names_out()[word_idx]}\")\n",
    "    \n",
    "    # Categorize the topic\n",
    "    category = categorize_topic(topic_dist, top_words, vectorizer)\n",
    "    print(f\"Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23835524-5150-40fd-a972-c91fbdc73eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0: Sentiment Score = 0.6\n",
      "Aspect 1: Sentiment Score = 0.6\n",
      "Aspect 2: Sentiment Score = 0.6\n",
      "Aspect 3: Sentiment Score = 0.6\n",
      "Aspect 4: Sentiment Score = 0.6\n",
      "Aspect 5: Sentiment Score = 0.6\n",
      "Aspect 6: Sentiment Score = 0.6\n",
      "Aspect 7: Sentiment Score = 0.6\n",
      "Aspect 8: Sentiment Score = 0.6\n",
      "Aspect 9: Sentiment Score = 0.6\n",
      "Aspect 10: Sentiment Score = 0.6\n",
      "Aspect 11: Sentiment Score = 0.6\n",
      "Aspect 12: Sentiment Score = 0.6\n",
      "Aspect 13: Sentiment Score = 0.6\n",
      "Aspect 14: Sentiment Score = 0.6\n",
      "Aspect 15: Sentiment Score = 0.6\n",
      "Aspect 16: Sentiment Score = 0.6\n",
      "Aspect 17: Sentiment Score = 0.6\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns a sentiment score between -1 (negative) and 1 (positive)\n",
    "\n",
    "# Extract topics and calculate sentiment\n",
    "aspect_sentiments = {}\n",
    "for i, document in enumerate(documents):\n",
    "    for topic_idx, topic_dist in enumerate(topic_word_distributions):\n",
    "        top_words = np.argsort(topic_dist)[::-1][:10]\n",
    "        aspect_keywords = [vectorizer.get_feature_names_out()[word_idx] for word_idx in top_words]\n",
    "        \n",
    "        # Check if document contains any top words for the current topic\n",
    "        if any(word in document for word in aspect_keywords):\n",
    "            sentiment = analyze_sentiment(document)\n",
    "            aspect_sentiments[f\"Aspect {topic_idx}\"] = sentiment\n",
    "\n",
    "# Print aspect-based sentiments\n",
    "for aspect, sentiment in aspect_sentiments.items():\n",
    "    print(f\"{aspect}: Sentiment Score = {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39583eca-1830-4bfe-97a4-a185b40671b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
